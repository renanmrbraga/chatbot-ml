# backend/scraping/scraper.py
from utils.logger import get_logger
from scraping.scraper_sidra_ibge import run as run_sidra
from scraping.extrator_basico_microdados import run as run_basico
from scraping.extrator_tecnico_microdados import run as run_tecnico
from scraping.city_data_sidra_to_csv import run as run_to_csv
from scraping.insert_database import run as run_insert

logger = get_logger(__name__)

def main():
    logger.info("ğŸš€ Iniciando pipeline de scraping completo...")

    etapas = [
        ("ğŸ” Etapa 1: Coleta do SIDRA (IBGE)", run_sidra),
        ("ğŸ“š Etapa 2: ExtraÃ§Ã£o de microdados educacionais (bÃ¡sico)", run_basico),
        ("ğŸ› ï¸ Etapa 3: ExtraÃ§Ã£o de dados tÃ©cnicos", run_tecnico),
        ("ğŸ“„ Etapa 4: GeraÃ§Ã£o do CSV final", run_to_csv),
        ("ğŸ“¥ Etapa 5: InserÃ§Ã£o no banco de dados", run_insert),
    ]

    for titulo, func in etapas:
        logger.info(titulo)
        try:
            func()
        except Exception as e:
            logger.critical(f"âŒ Falha na etapa: {titulo} â†’ {e}")
            logger.critical("ğŸ›‘ Pipeline encerrado devido a erro crÃ­tico.")
            break
    else:
        logger.success("âœ… Pipeline executado com sucesso! Dados atualizados.")

if __name__ == "__main__":
    main()
